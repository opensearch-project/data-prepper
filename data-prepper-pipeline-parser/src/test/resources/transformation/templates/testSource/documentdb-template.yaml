"{{pipeline-name}}-transformed":
  workers: 2
  delay: 0
  source: "{{$.*.source}}"
  buffer:
    bounded_blocking:
      batch_size: 125000
      buffer_size: 1000000
  route:
    - initial_load: 'getMetadata("ingestion_type") == "EXPORT"'
    - stream_load: 'getMetadata("ingestion_type") == "STREAM"'
  sink:
    - s3:
        routes:
          - initial_load
        aws: "{{$.*.sink[?(@.opensearch)].opensearch.aws}}"
        bucket: "{{$.*.source.documentdb.collections[0].s3_bucket}}"
        threshold:
          event_collect_timeout: "120s"
          maximum_size: "2mb"
        aggregate_threshold:
          maximum_size: "256mb"
          flush_capacity_ratio: "0"
        object_key:
          path_prefix: "local-test/${getMetadata(\"s3_partition_key\")}"
        codec:
          json:
    - s3:
        routes:
          - stream_load
        aws: "{{$.*.sink[?(@.opensearch)].opensearch.aws}}"
        bucket: "{{$.*.source.documentdb.collections[0].s3_bucket}}"
        threshold:
          event_collect_timeout: "30s"
          maximum_size: "1mb"
        aggregate_threshold:
          maximum_size: "128mb"
          flush_capacity_ratio: 0
        object_key:
          path_prefix: "local-test/${getMetadata(\"s3_partition_key\")}"
        codec:
          json:

"{{pipeline-name}}-s3-sub-pipeline-transformed":
  workers: 2
  delay: 0
  source:
    s3:
      delete_s3_objects_on_read: true
      codec:
        json: # Other options "json", "csv", "parquet"
      compression: "none"
      aws: "{{$.*.sink[?(@.opensearch)].opensearch.aws}}"
      acknowledgments: false
      scan:
        buckets:
          - bucket:
              name: "{{$.*.source.documentdb.collections[0].s3_bucket}}"
              filter:
                include_prefix: ["local-test"]
        scheduling:
          interval: "1s"
  processor: "{{$.*.processor}}"
  buffer:
    bounded_blocking:
      batch_size: 125000
      buffer_size: 1000000
  sink:
    - opensearch: "{{$.*.sink[?(@.opensearch)].opensearch}}"